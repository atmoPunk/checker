Данный алгоритм заключается в следующем:

Исходный код программы студента конкатенируется в один файл:
Сперва идут заголовочные файлы, затем файлы с имплементацией
В процессе обхода директивы #include, которые указвыают на 
файлы стандартной библиотеки помечаются специальными маркерами,  // Сейчас помечаются #include <...>
до и после самой директивы, а в инклюды на файлы, написанные студентом,
препроцессор заходит, запоминает что этот файл мы уже включили
и вставляет текст файла на место инклюда (по пути так же обходя инклюды).
Получается так, что наш препроцессор выполняет часть работы препроцессора C/C++ по исполнению директивы #include
Это позволяет нам избавиться от проблем с повторным включения одного и того же файла.

Затем при помощи команды clang -Xclang -ast-dump -fsyntax-only -fno-color-diagnostics -w
строится абстрактное синтаксическое дерево всей программы. Так как clang включает код
из файлов, которые были включены при помощи #include, то он строит дерево из файлов,
входящих в стандартную библиотеку. Нам эти файлы не нужны, поэтому из дерева вырезаются
куски которые были помечены маркерами на прошлом шаге.

Так же из дерева вырезается информация об адресах(?) (построенных clang),
о местоположениях в файлах (строки и колонки), и идентификаторах.
Это позволяет сопоставить фрагменты, где например переменная была просто переименнована,
но в то же время теряется часть информации. Для избежания этого можно не просто удалять
информацию о идентификаторах, а заменять её у всех студентов на одинаковые // Ещё не сделано
(Для каждого top-level (узел, который на один уровень ниже TranslationUnitDecl - корень дерева)
узла заменять int a; int b на int int1; int int2; и т.п.
Так как нумерация будет начинаться сначала на каждом top-level узле, то
перестановка местами двух имплементаций функций не помешает нашему анализатору)
Скорее всего это увеличит точность

После всех этих действий для трех, подряд (в том порядке, в котором
их выдает clang) идущих узла дерева строится хэш от их строчного представления
(сейчас используется функция md5), и все эти хэши записываются в словарь и подсчитывается количество
их появлений.

Для сравнения двух работ можно просто сравнить два словаря - посмотреть,
насколько много хэшей они имеют в общем.
Это значение может показать насколько работы похожи, но
оно не учитывает того, что во многих работах существуют куски,
которые будут повторяться всегда, даже если никакого плагиата там нет.

Для избежания этой проблемы можно построить индекс хэшей к работам для 
данного задания. (Для каждого хэша сообщить, в каких файлах он встречается)
Это позволит взвесить хэши, и придавать им меньше значения (если он встречается
почти везде), или больше (если встречается редко). (В работе Same Differnence: Detecting Collusion
by Finding Unusual Shared Elements сравнивались пары файлов и учитывались 
совпадения триграмм с весом 1, если они встречались только в этой паре файлов, с весом 0.5,
если они встречались в этой паре и в каком нибудь ещё, с весом 0.33, если они встречались в 4
файлах, считая данную пару. Если триграмма встречалась больше чем в 4 файлах, она считалась
слишком популярной, и её вес не учитывался)
